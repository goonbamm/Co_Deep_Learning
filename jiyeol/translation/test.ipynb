{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab11/anaconda3/envs/cdl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/nlplab11/anaconda3/envs/cdl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/nlplab11/anaconda3/envs/cdl/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/nlplab11/anaconda3/envs/cdl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils import set_seed, print_trainable_parameters\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig, Trainer,\n",
    "                          TrainingArguments, DataCollatorForLanguageModeling)\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytes Config for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'beomi/kollama-13b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TOKENIZER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    use_fast=False,\n",
    "    truncation_side='left',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'checkpoint-240'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 28/28 [00:05<00:00,  4.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            device_map='auto')\n",
    "\n",
    "# PREPARE MODEL FOR K BIT TRAINING\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config\n",
    "config = LoraConfig.from_json_file(f'outputs/{model_checkpoint}/adapter_config.json')\n",
    "\n",
    "model = PeftModel.from_pretrained(model, model_id=f'outputs/{model_checkpoint}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  5 10:44:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 86%   71C    P2   342W / 350W |  23079MiB / 24268MiB |     94%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:21:00.0 Off |                  N/A |\n",
      "| 39%   43C    P2   111W / 350W |  10509MiB / 24268MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 39%   40C    P8    18W / 350W |  20113MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:42:00.0 Off |                  N/A |\n",
      "| 38%   30C    P8    24W / 350W |   4117MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11161      C   ...onda3/envs/cdl/bin/python    23071MiB |\n",
      "|    0   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      5229      C   ...onda3/envs/cdl/bin/python    10501MiB |\n",
      "|    1   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A     49946      C   ...nvs/clip4clip/bin/python3    20105MiB |\n",
      "|    2   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A     49946      C   ...nvs/clip4clip/bin/python3     4109MiB |\n",
      "|    3   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(52000, 5120, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=52000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    'Below is an instruction that describes a task, paired with an input that provides further context.',\n",
    "    '아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n',\n",
    "    'Write a response that appropriately completes the request.',\n",
    "    '요청을 적절히 완료하는 응답을 작성하세요.\\n',\n",
    "    '### Instruction(명령어):',\n",
    "    '아이의 요구사항을 그림 생성을 위한 프롬프트를 번역하세요.\\n',\n",
    "    '### Input(입력):',\n",
    "    '아이: 17세기 도둑이 물 속에 다친 회색 말을 보며 슬퍼하는 모습을 담은 그림을 그려봐. 그리고 찬 영국 공기에 바람에 흔들리는 갈대와 망토를 넣어줘.\\n',\n",
    "    '### Response(응답):'\n",
    "    ]\n",
    "\n",
    "inputs = tokenizer('\\n'.join(prompt), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "아이의 요구사항을 그림 생성을 위한 프롬프트를 번역하세요.\n",
      "\n",
      "### Input(입력):\n",
      "아이: 17세기 도둑이 물 속에 다친 회색 말을 보며 슬퍼하는 모습을 담은 그림을 그려봐. 그리고 찬 영국 공기에 바람에 흔들리는 갈대와 망토를 넣어줘.\n",
      "\n",
      "### Response(응답):\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "아이의 요구사항을 그림 생성을 위한 프롬프트를 번역하세요.\n",
      "\n",
      "### Input(입력):\n",
      "아이: 17세기 도둑이 물 속에 다친 회색 말을 보며 슬퍼하는 모습을 담은 그림을 그려봐. 그리고 찬 영국 공기에 바람에 흔들리는 갈대와 망토를 넣어줘.\n",
      "\n",
      "### Response(응답):Wide-Angle Art a 16th century painting of a 17thcentury man with a fractured white hair, she is sleeping in a dark woods, \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'].to('cuda'),\n",
    "                             max_length=256,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             early_stopping=True)\n",
    "    outputs = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce160728c6af9d583e29731fd790ca41f832d7028e09ba1effb18caae33e60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
