{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHX-RCPdkXgP",
        "outputId": "d294f833-6f45-4014-e4ed-75ef26141a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jun 22 17:38:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
            "| 40%   31C    P8    23W / 350W |      8MiB / 24268MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA GeForce ...  On   | 00000000:21:00.0 Off |                  N/A |\n",
            "| 39%   30C    P8    24W / 350W |      8MiB / 24268MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\n",
            "| 39%   30C    P8    18W / 350W |      8MiB / 24268MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  NVIDIA GeForce ...  On   | 00000000:42:00.0 Off |                  N/A |\n",
            "| 38%   28C    P8    23W / 350W |      8MiB / 24268MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    1   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    2   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "|    3   N/A  N/A     60973      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuXIFTFapAMI",
        "outputId": "340f0ca7-6dfd-4350-b2d2-15f983adfc66"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ-5idQwzvg-"
      },
      "source": [
        "First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "278070bfaaac4794a649f11306ad2947",
            "5391d6d2385549319099d3bf653c34ee",
            "ee696f00718b43989f653dd01391a87d",
            "e39e4f5284224bfdbde4359d8593d206",
            "0a91d94890634dc1bb43176347c27fec",
            "ec9d84f9337948cba5562b7e995bb2ef",
            "3473e65596d5455385269f59e6159e38",
            "416d78e5558f4019972720c8d52a410b",
            "388757e992344b06b2fc7e4573eba77d",
            "664f6a3720a446929197650580dd005d",
            "92e0594143bd4c2690dc3ddaa0170b17"
          ]
        },
        "id": "jm4FzCvfeYcK",
        "outputId": "bf40f8d2-7603-407d-fba5-6686eddd49aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading readme: 1.75kB [00:00, 1.92MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset None/None to /home/nlplab11/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 12.9M/12.9M [00:00<00:00, 86.2MB/s]\n",
            "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
            "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1344.76it/s]\n",
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset parquet downloaded and prepared to /home/nlplab11/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 796.34it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"beomi/KoAlpaca-v1.1a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KUhV7x3e6Db",
        "outputId": "1fdf62ba-5269-435c-dd11-29a381ba04c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'output', 'url'],\n",
              "        num_rows: 21155\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FbgsI9sezTJ",
        "outputId": "7df84efa-917f-4d13-e5ba-62fc1d0b3d25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        }
      ],
      "source": [
        "data = data.map(\n",
        "    lambda x: {'text': f\"### 질문: {x['instruction']}\\n\\n### 답변: {x['output']}<|endoftext|>\" }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Nl5mWL0k2T",
        "outputId": "e601d45a-99f5-46ff-aecf-497ed8754c24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 231/231 [00:00<00:00, 55.4kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 2.69MB [00:00, 3.80MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 93.0/93.0 [00:00<00:00, 30.5kB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 507/507 [00:00<00:00, 575kB/s]\n",
            "Downloading (…)fetensors.index.json: 26.8kB [00:00, 48.4MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 965M/965M [00:10<00:00, 89.2MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 990M/990M [00:12<00:00, 80.8MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 967M/967M [00:11<00:00, 84.4MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 967M/967M [00:11<00:00, 87.7MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 990M/990M [00:17<00:00, 57.1MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 944M/944M [00:10<00:00, 88.0MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 990M/990M [00:11<00:00, 86.1MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 967M/967M [00:11<00:00, 83.6MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 967M/967M [00:13<00:00, 72.3MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 990M/990M [00:10<00:00, 98.2MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 944M/944M [00:09<00:00, 101MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 990M/990M [00:16<00:00, 58.6MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 967M/967M [00:09<00:00, 99.5MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 742M/742M [00:10<00:00, 71.6MB/s]\n",
            "Downloading (…)of-00015.safetensors: 100%|██████████| 426M/426M [00:05<00:00, 82.9MB/s]\n",
            "Downloading shards: 100%|██████████| 15/15 [03:02<00:00, 12.16s/it]\n",
            "/home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/nlplab11/anaconda3/envs/jyp/lib/libcudart.so.11.0'), PosixPath('/home/nlplab11/anaconda3/envs/jyp/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
            "CUDA SETUP: CUDA runtime path found: /home/nlplab11/anaconda3/envs/jyp/lib/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 113\n",
            "CUDA SETUP: Loading binary /home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n",
            "Downloading (…)neration_config.json: 100%|██████████| 137/137 [00:00<00:00, 46.2kB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"beomi/kollama-7b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      },
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4TDUgDbhyhK",
        "outputId": "8aac9a8c-5211-4cb8-bd43-3b99b24c9c00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        }
      ],
      "source": [
        "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybeyl20n3dYH",
        "outputId": "327a4a63-db3d-4631-edec-23f8d08d14ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4194304 || all params: 3668447232 || trainable%: 0.11433458721752714\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FCc64bfnmd3j"
      },
      "source": [
        "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MOtwf3zdZp"
      },
      "source": [
        "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jq0nX33BmfaC",
        "outputId": "769fb34e-acd0-4a56-ac87-40db98dabd78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdysunleo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/media/nlplab11/jyp/Co_Deep_Learning/jiyeol/wandb/run-20230622_174454-u2wo1nss</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kdysunleo/huggingface/runs/u2wo1nss' target=\"_blank\">young-universe-1</a></strong> to <a href='https://wandb.ai/kdysunleo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kdysunleo/huggingface' target=\"_blank\">https://wandb.ai/kdysunleo/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kdysunleo/huggingface/runs/u2wo1nss' target=\"_blank\">https://wandb.ai/kdysunleo/huggingface/runs/u2wo1nss</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:50, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.864800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.455500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.298400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.400300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.488400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.257100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.426500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.359700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.538800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.528400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>3.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>3.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.586500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>3.473600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.418100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>3.648000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>3.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>3.342700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>3.526400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>3.560700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>3.570500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>3.233800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>3.425900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.174300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>3.388000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>3.502000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>3.476500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>3.424500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>3.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>3.307400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>3.311100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>3.427200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.074300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>3.325700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>3.408200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>3.228700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>3.265100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.215100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>3.299700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>3.123100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>3.282400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>3.292300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.140500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=3.394790828704834, metrics={'train_runtime': 419.9929, 'train_samples_per_second': 2.381, 'train_steps_per_second': 1.19, 'total_flos': 4917472162480128.0, 'train_loss': 3.394790828704834, 'epoch': 0.05})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "# needed for gpt-neo-x tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # warmup_steps=200,\n",
        "        max_steps=500, ## 초소형만 학습: 10 step = 20개 샘플만 학습.\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'### 질문: 양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\\n\\n### 답변: 양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.<|endoftext|>'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"train\"][\"text\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvP11yUbhzWP",
        "outputId": "96eb0cfe-e22f-4642-9fba-796942c62540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wow\n"
          ]
        }
      ],
      "source": [
        "print(\"wow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a-jauOEv9XVe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ITTiXfp-2r",
        "outputId": "e6376ea8-f293-4ca4-f9f3-13f231f5de07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/transformers/generation/utils.py:1355: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/transformers/generation/utils.py:1454: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[14831,  6792,    29,  1933, 42731,    34,   202,   202, 14831,  5994,\n",
              "            29,  1933, 42731,  4210, 25388,   489,    17,  2019,    15,  1933]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(**tokenizer(\"### 질문: 오늘 날씨는?\", return_tensors='pt', return_token_type_ids=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oDp9W-Gmp5Mb"
      },
      "outputs": [],
      "source": [
        "def gen(x):\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            f\"### 질문: {x}\\n\\n### 답변:\",\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ),\n",
        "        max_new_tokens=256,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(tokenizer.decode(gened[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "iIbK1GaipZd9",
        "outputId": "46537a15-3bce-4959-860a-80706a8b3693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### 질문: 건강하게 살기 위한 세 가지 방법은?\n",
            "\n",
            "### 답변: 건강하게 살기 위한 세 가지 방법은 어떤 종류의 세 가지가 있을 지 예상이 어렵습니다.  \n",
            "\n",
            "1. 건강하게 살기 위한 세 가지 방법은 일반적으로 체중, 체중, 체중 등으로 구성되는 세 가지 방법입니다.  \n",
            "2. 건강하게 살기 위한 세 가지 방법은 대체로 체중이나 체중을 관리하기 위한 방법들로 나누어집니다. \n",
            "3. 건강하게 살기 위한 세 가지 방법과 체중이나 체중 등에 대한 정보를 교환하기 위한 방안으로는 방법들이 있습니다. \n",
            "4. 건강하게 살기 위한 세 가지 방법은 전반적으로 신체 건강 관리에 힘쓰는 방법들로 구별됩니다. \n",
            "5. 건강하게 살기 위한 세 가지 방법도 기본적으로 체중, 체중, 신체 건강 관리에 힘쓰는 방법들로 나누어집니다. \n",
            "6. 건강을 유지하고, 잘 돌보아 주기 위한 세 가지는 다음과 같습니다. \n",
            "7. 건강하게 살기 위한 세 가지 방법은 크게 두 가지 방식으로 나뉩니다. 1. 건강하게 살기 위한 세 가지 방법: 체중과 체중. 체중으로 구성되는 세 가지 방법이 있습니다. 체중을 관리하기 위해서는 체중과 체중이 모두 포함되어 있어야 한다는 점에 유의하여 적당하게 체중을 나누어 체중과 체중을 나누어 건강 관리를 해 나가야 합니다. 체중을 나누\n"
          ]
        }
      ],
      "source": [
        "gen('건강하게 살기 위한 세 가지 방법은?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "1YOtvI6rpuoP",
        "outputId": "949faae7-4a6b-4074-b408-6259f696c6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### 질문: 슈카월드가 무엇인가요?\n",
            "\n",
            "### 답변: 슈카월은 영국 프로방드에서 3월부터 6월 초까지 활동했던 '호퍼'입니다. 그의 나이 40세입니다. 그의 이름은 '비올라나'(E.A.C.A.R.)입니다. \n",
            "\n",
            "슈카월은 프랑스의 피파마, 아탈란타, 툴루즈, 스카리브, 리나(S.A.R.R.R.) 등이 소속돼 있지만, 현재는 '알레데스타'가 되어 있습니다. \n",
            "\n",
            "샤방샤방슈카월은 \"알로슈(Fruses)\", 샤르노(S.A.R.S)), 세리에A(S.A.C.A.R.) 등과 관련된 문헌이 있습니다. \n",
            "\n",
            "알로슈는 주로 '오스카' 박하나 베하스(O.Baelus), '오스카(O.C.A.C.A.C.A.R)', '오셔니도비드', '오셔니도비드', '오셔니도비드', '베드너박', '오가사제(D.I.O.C.A.I.O.C.A.I.O\n"
          ]
        }
      ],
      "source": [
        "gen('슈카월드가 무엇인가요?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "oaNYJD81sL75",
        "outputId": "bc79e6f9-6144-447c-dd3e-fead9203d64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### 질문: 공공성을 측정하기위한 방법은?\n",
            "\n",
            "### 답변: 한국에는 철골구조가 없어서 건설공사의 중력과 균형을 맞추기 위한 철골조 구조물에 비해, 철골구조물의 깊이가 깊을수록 철골 구조물의 단면을 측정하는 것은 어렵고 정확합니다. 이는 철골조 건축구조물에 비해 더욱 정확합니다. 단면적으로만 봤을 때, 철골조 건축구조물에 비해 정밀도를 향상시킨다는 뜻인 'A2'는 일반적으로 철골구조물의 단면을 고려하지 않고 설계하여 만들어진 공법으로, 건축구조물의 단면상 높이와는 관계 없이 철골 구조물에 비해 정밀한 측정력과 정확한 안전성을 가집니다. \n",
            "\n",
            "건물의 각 층마다 두께의 단면상 차이가 있지만, 단면으로 올라가는 철골조 건축구조물은 층고 상한, 바닥 두께 등 철골구조의 단면성에 비해 정밀한 구조체도를 갖게 됩니다.\n",
            "\n",
            "또한, 단면으로 올라가는 철골조 건축구조는 철골구조 건축물의 단면상에서 가장 높은 단면이 되고, 단면만이 아닌 구조체의 단면을 보여주는 것을 특징으로 하며, 단면은 주로 건물의 높낮이를 측정하여 단면과 단면이 이루는 높이와 단면을 고려하여 산출됩니다.\n",
            "\n",
            "그리고, 철골구조 건축물에서 단면상보다 단면상\n"
          ]
        }
      ],
      "source": [
        "gen('공공성을 측정하기위한 방법은?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOWSgQ6RuDK3",
        "outputId": "924e3e5f-ea24-4e6c-bf49-9665e5eb804c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### 질문: 주식 시장에서 안정적으로 수익을 얻기 위한 방법은?\n",
            "\n",
            "### 답변: 주식시장 거래소는 증권과 증권사, 투신, 증권보험이 가장 널리 사용되고 있지만, 증권사만 표준화된 용어로 사용되고 있습니다. 일반적으로 주식 거래소는 증권 주식과 달리 계좌를 이용하기 때문에 증권 주식에서 발행되는 모든 종목은 증권의 가치와 시장 가치를 함께 평가합니다. 따라서 종목마다 동일한 자산일지를 기준으로 투자하려는 노력을 기울이기 위해, 가장 적합한 상품 선택 방법을 통해 증권 회사와 거래소 모두에게 공통적으로 적합한 상품 선택 방법을 도울 예정입니다. \n",
            "\n",
            "중반부에 자산 배분과 시장성이 확인되면, 가장 먼저 증권 업체가 가장 적합한 상품의 선택 방법을 찾습니다. 우선 중세기에 발달한 기술에서 증권 증권 기업이 활용되어왔고, 현재는 증권, 상사 및 예탁결제, ELS 상품 등이 대표적입니다. 이러한 자산 구성을 위해서는 증권 종목 선택 방법과 증권사 선택 방법에 대하여 공부합니다. 증권 회사와 거래소는 각각에 따라 다를 수 있으며, 반드시 둘 중 하나일 것 같습니다. \n",
            "\n",
            "투자와 관련된 증권의 종류와 상품을 선택할 수 있는 선택 방식을 고민하고, 증권 회사와 거래소는 서로에게 가장 정확한 정보를 제공할 수 있는 상품들을 찾아보세요. 증권 계좌에 대한 설명을 자세히 통해 증권사에게 가장 적합한 상품 선택 방법을 알려줍니다. 또한, 증권 계좌를 선택하는 방법은 증권사마다 차이가 큰\n"
          ]
        }
      ],
      "source": [
        "gen('주식 시장에서 안정적으로 수익을 얻기 위한 방법은?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nlplab11/anaconda3/envs/jyp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Found cached dataset csv (/home/nlplab11/.cache/huggingface/datasets/csv/default-6ae17a99ca4cc402/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100%|██████████| 1/1 [00:00<00:00, 840.21it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset('csv', data_files='../preprocess/train.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a91d94890634dc1bb43176347c27fec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278070bfaaac4794a649f11306ad2947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5391d6d2385549319099d3bf653c34ee",
              "IPY_MODEL_ee696f00718b43989f653dd01391a87d",
              "IPY_MODEL_e39e4f5284224bfdbde4359d8593d206"
            ],
            "layout": "IPY_MODEL_0a91d94890634dc1bb43176347c27fec"
          }
        },
        "3473e65596d5455385269f59e6159e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "388757e992344b06b2fc7e4573eba77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "416d78e5558f4019972720c8d52a410b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5391d6d2385549319099d3bf653c34ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9d84f9337948cba5562b7e995bb2ef",
            "placeholder": "​",
            "style": "IPY_MODEL_3473e65596d5455385269f59e6159e38",
            "value": "100%"
          }
        },
        "664f6a3720a446929197650580dd005d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e0594143bd4c2690dc3ddaa0170b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e39e4f5284224bfdbde4359d8593d206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_664f6a3720a446929197650580dd005d",
            "placeholder": "​",
            "style": "IPY_MODEL_92e0594143bd4c2690dc3ddaa0170b17",
            "value": " 1/1 [00:00&lt;00:00,  9.41it/s]"
          }
        },
        "ec9d84f9337948cba5562b7e995bb2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee696f00718b43989f653dd01391a87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416d78e5558f4019972720c8d52a410b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_388757e992344b06b2fc7e4573eba77d",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
