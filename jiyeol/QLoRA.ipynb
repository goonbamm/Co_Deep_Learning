{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XIyP_0r6zuVc"
      },
      "source": [
        "# `transformers` meets `bitsandbytes` for democratzing Large Language Models (LLMs) through 4bit quantization\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/huggingface/blog/blob/main/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png?raw=true\" alt=\"drawing\" width=\"700\" class=\"center\"/>\n",
        "</center>\n",
        "\n",
        "Welcome to this notebook that goes through the recent `bitsandbytes` integration that includes the work from XXX that introduces no performance degradation 4bit quantization techniques, for democratizing LLMs inference and training.\n",
        "\n",
        "In this notebook, we will learn together how to load a large model in 4bit (`gpt-neo-x-20b`) and train it using Google Colab and PEFT library from Hugging Face ðŸ¤—.\n",
        "\n",
        "[In the general usage notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing), you can learn how to propely load a model in 4bit with all its variants.\n",
        "\n",
        "If you liked the previous work for integrating [*LLM.int8*](https://arxiv.org/abs/2208.07339), you can have a look at the [introduction blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) to lean more about that quantization method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHX-RCPdkXgP",
        "outputId": "d294f833-6f45-4014-e4ed-75ef26141a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jun 22 16:51:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   40C    P8    31W / 340W |    429MiB / 10240MiB |     35%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1139      G   /usr/lib/xorg/Xorg                 35MiB |\n",
            "|    0   N/A  N/A      1839      G   /usr/lib/xorg/Xorg                102MiB |\n",
            "|    0   N/A  N/A      1972      G   /usr/bin/gnome-shell               40MiB |\n",
            "|    0   N/A  N/A      2928      G   ...189396807265038139,262144      105MiB |\n",
            "|    0   N/A  N/A      3626      G   ...RendererForSitePerProcess      129MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuXIFTFapAMI",
        "outputId": "340f0ca7-6dfd-4350-b2d2-15f983adfc66"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ-5idQwzvg-"
      },
      "source": [
        "First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "278070bfaaac4794a649f11306ad2947",
            "5391d6d2385549319099d3bf653c34ee",
            "ee696f00718b43989f653dd01391a87d",
            "e39e4f5284224bfdbde4359d8593d206",
            "0a91d94890634dc1bb43176347c27fec",
            "ec9d84f9337948cba5562b7e995bb2ef",
            "3473e65596d5455385269f59e6159e38",
            "416d78e5558f4019972720c8d52a410b",
            "388757e992344b06b2fc7e4573eba77d",
            "664f6a3720a446929197650580dd005d",
            "92e0594143bd4c2690dc3ddaa0170b17"
          ]
        },
        "id": "jm4FzCvfeYcK",
        "outputId": "bf40f8d2-7603-407d-fba5-6686eddd49aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset parquet (/home/jyp/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34de3f7e715b4aaca1e3bb115b21365b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"beomi/KoAlpaca-v1.1a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KUhV7x3e6Db",
        "outputId": "1fdf62ba-5269-435c-dd11-29a381ba04c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'output', 'url'],\n",
              "        num_rows: 21155\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FbgsI9sezTJ",
        "outputId": "7df84efa-917f-4d13-e5ba-62fc1d0b3d25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/jyp/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-16bb9a5cce0615fd.arrow\n"
          ]
        }
      ],
      "source": [
        "# # data\n",
        "# data = data.map(\n",
        "#     lambda x:\n",
        "#     {'text': f\"### ëª…ë ¹ì–´: {x['instruction']}\\n\\n###ë§¥ë½: {x['input']}\\n\\n### ë‹µë³€: {x['output']}<|endoftext|>\" }\n",
        "#     if x['input'] else\n",
        "#     {'text':f\"### ëª…ë ¹ì–´: {x['instruction']}\\n\\n### ë‹µë³€: {x['output']}<|endoftext|>\"},\n",
        "# )\n",
        "# data\n",
        "data = data.map(\n",
        "    lambda x: {'text': f\"### ì§ˆë¬¸: {x['instruction']}\\n\\n### ë‹µë³€: {x['output']}<|endoftext|>\" }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Nl5mWL0k2T",
        "outputId": "e601d45a-99f5-46ff-aecf-497ed8754c24"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a200a9897734eb893ba066b52efd583",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/231 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe1a8de5357847acb540f82b4a2c45de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a7157f00e9948d9993f1b7fe728429e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9020169462e4b9a8b55084016c003d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d83f16a0c394bd48e2a9430857924f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)fetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "879917c172434184b74e7f777a086136",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af516d17d43c49a9b5dd305713bb5ef3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/965M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa577ba34b704092953746d670e75636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2984219ea1242e5a4398ff7aac37a41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d0d9a1bb6542c4aa786898abcd8782",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62191fdba9a644caa5910bc47f9af7cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2f580e78d8c47689d0dfd5b9170a3fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d02cdd2c94424ebca65797268907ef72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e63363c7af444e9594a55a40e9723b93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ddc82d51204480a537c46f522e4c1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1329f7e93ae74b39a279d004ddcb8d7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a08c46b6cfa74968a090c174abe7a014",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/944M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97b502c523a94881bb1da240cd260e81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1acc3458f6344315a4d230f031b133a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e31f514a1b748d5bd7a8a92cf3d7c20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/742M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db72c70291e1492e96d2cac5ae4cfd7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)of-00015.safetensors:   0%|          | 0.00/426M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /home/jyp/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.7/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 117\n",
            "CUDA SETUP: Loading binary /home/jyp/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e06ea2ecc51846fe95846cec692bc0bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a661738a97fc4460a1ca2e474966ad57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"beomi/kollama-7b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      },
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4TDUgDbhyhK",
        "outputId": "8aac9a8c-5211-4cb8-bd43-3b99b24c9c00"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "175b6abb2d8c466aadee402c6d51e86c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybeyl20n3dYH",
        "outputId": "327a4a63-db3d-4631-edec-23f8d08d14ac"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Target modules ['query_key_value'] not found in the base model. Please check the target modules and try again.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m      3\u001b[0m config \u001b[39m=\u001b[39m LoraConfig(\n\u001b[1;32m      4\u001b[0m     r\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[1;32m      5\u001b[0m     lora_alpha\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCAUSAL_LM\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model \u001b[39m=\u001b[39m get_peft_model(model, config)\n\u001b[1;32m     13\u001b[0m print_trainable_parameters(model)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/mapping.py:122\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    121\u001b[0m     peft_config \u001b[39m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39;49mtask_type](model, peft_config, adapter_name\u001b[39m=\u001b[39;49madapter_name)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/peft_model.py:798\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, peft_config, adapter_name)\n\u001b[1;32m    799\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/peft_model.py:106\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name] \u001b[39m=\u001b[39m peft_config\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model \u001b[39m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39;49mpeft_type](\n\u001b[1;32m    107\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpeft_config, adapter_name\n\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/tuners/lora.py:174\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 174\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_adapter(adapter_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpeft_config[adapter_name])\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/tuners/lora.py:181\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    179\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_lora_config(config, model_config)\n\u001b[1;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name] \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 181\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_and_replace(adapter_name)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name]\u001b[39m.\u001b[39mbias \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for all adapters.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/tuners/lora.py:333\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget modules \u001b[39m\u001b[39m{\u001b[39;00mlora_config\u001b[39m.\u001b[39mtarget_modules\u001b[39m}\u001b[39;00m\u001b[39m not found in the base model. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease check the target modules and try again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Target modules ['query_key_value'] not found in the base model. Please check the target modules and try again."
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Module.named_parameters of LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(52000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (1): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (2): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (3): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (4): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (5): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (6): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (7): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (8): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (9): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (10): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (11): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (12): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (13): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (14): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (15): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (16): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (17): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (18): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (19): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (20): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (21): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (22): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (23): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (24): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (25): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (26): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (27): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (28): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (29): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (30): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (31): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=52000, bias=False)\n",
              ")>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.named_parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FCc64bfnmd3j"
      },
      "source": [
        "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6f4z8EYmcJ6",
        "outputId": "21c8ba2d-fd89-40f3-f0b6-c73772f06bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May 30 05:21:32 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    30W /  70W |   9435MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MOtwf3zdZp"
      },
      "source": [
        "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jq0nX33BmfaC",
        "outputId": "769fb34e-acd0-4a56-ac87-40db98dabd78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 47:02, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.940200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.895000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.951200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.898600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.919400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.092700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.817300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.875600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.151900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.900700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.901100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.264800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.938400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.987100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.983300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.997100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.995700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.995900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.868700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.915200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.878300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.877700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.931400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.925000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.933100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.895300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.895000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.908200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.903600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.768100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.740700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.843200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.993800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.751400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.783000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.890400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.891800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.907500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.9244559707641602, metrics={'train_runtime': 2828.6026, 'train_samples_per_second': 0.354, 'train_steps_per_second': 0.177, 'total_flos': 1.103736813379584e+16, 'train_loss': 1.9244559707641602, 'epoch': 0.05})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "# needed for gpt-neo-x tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # warmup_steps=200,\n",
        "        max_steps=500, ## ì´ˆì†Œí˜•ë§Œ í•™ìŠµ: 10 step = 20ê°œ ìƒ˜í”Œë§Œ í•™ìŠµ.\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvP11yUbhzWP",
        "outputId": "96eb0cfe-e22f-4642-9fba-796942c62540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wow\n"
          ]
        }
      ],
      "source": [
        "print(\"wow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-jauOEv9XVe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ITTiXfp-2r",
        "outputId": "e6376ea8-f293-4ca4-f9f3-13f231f5de07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1349: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[   6,    6,    6, 2438,   29, 1832, 4770,  272,   34,  224,  202, 4588,\n",
              "         4770,  272, 1382, 5674,   34, 5000, 9357,  272]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(**tokenizer(\"### ì§ˆë¬¸: ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?\", return_tensors='pt', return_token_type_ids=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDp9W-Gmp5Mb"
      },
      "outputs": [],
      "source": [
        "def gen(x):\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            f\"### ì§ˆë¬¸: {x}\\n\\n### ë‹µë³€:\",\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False\n",
        "        ),\n",
        "        max_new_tokens=256,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(tokenizer.decode(gened[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "iIbK1GaipZd9",
        "outputId": "46537a15-3bce-4959-860a-80706a8b3693"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'### ì§ˆë¬¸: ê±´ê°•í•˜ê²Œ ì‚´ê¸° ìœ„í•œ ì„¸ ê°€ì§€ ë°©ë²•ì€?\\n\\n### ë‹µë³€: 1. ê· í˜•ìžˆëŠ” ì‹ì‚¬: íŽ¸ì‹ì„ ë©€ë¦¬í•˜ê³ , ì±„ì†Œ, ê³¼ì¼, ìƒì„ ì„ ì„­ì·¨í•˜ë©´ ê±´ê°•í•´ì ¸ìš”.\\n\\n2. ê³¼ì‹ì„ ì¤„ì¼ ê²ƒ: ê³¼ì‹í•˜ê²Œ ë˜ë©´ ìœ„ìž¥ì´ í™œë°œí•˜ì—¬ì ¸ì„œ ë³‘ì´ ë‚˜ê¸° ì‰½ê²Œ ë˜ìš”.\\n3. ì ë‹¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤: ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ìŒ“ì•„ë‘ëŠ” ê²ƒì€ ë‚´ ì•ˆì— ë…ì´ ìŒ“ì´ëŠ” ê²ƒì´ë¼ì„œ ì˜¤ížˆë ¤ ê±´ê°•ì— ì¢‹ì§€ ì•Šì•„ìš”.\\n\\nì¦‰, ê· í˜•ìžˆëŠ” ì‹ì‚¬, ì ë‹¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ, ê³¼ì‹ì„ ì¤„ì´ë©´ ìš°ë¦¬ ëª¸ì— ì¢‹ìŠµë‹ˆë‹¤. ì´ ì„¸ ê°€ì§€ë¥¼ ìŠµê´€í™”í•˜ë©´ì„œ ê±´ê°•í•œ ì‚¶ì„ ìœ ì§€í•œë‹¤ë©´ ê±´ê°•í•˜ê²Œ ì‚´ ìˆ˜ ìžˆê²Œ ë  ê²ƒìž…ë‹ˆë‹¤. \\n\\nìœ„ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì€ ì´ˆë“±í•™ìƒ ë•Œë¶€í„° ë°°ìš°ë˜ ê²ƒì´ë¯€ë¡œ, ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ëª¨ë‘ ê±´ê°•í•˜ê²Œ ì‚¬ëŠ” ì§€ë¦„ê¸¸ì´ë¯€ë¡œ, ì´ë¥¼ ì‹¤ì²œí•˜ì—¬ ê±´ê°•í•œ ì‚¶ì„ ì‚´ ìˆ˜ ìžˆë„ë¡ ë…¸ë ¥í•©ì‹œë‹¤.\\n### ë‹µë³€ì„ ìž‘ì„±í•˜ëŠ”ë°, ì•½ê°„ì˜ ì‹œê°„ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤. ì§ˆë¬¸ ë‚´ìš©ì—ì„œë„ ì•Œ ìˆ˜ ìžˆëŠ” ê²ƒì²˜ëŸ¼, ê°„ë‹¨í•œ ë°©ë²•ì´ì§€ë§Œ ì‹¤í–‰ì— ì˜®ê¸°ê¸°ëŠ” ì‰¬ìš´ ì¼ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ë…¸ë ¥í•˜ë©° ì‹¤í–‰í•œë‹¤ë©´ ì¶©ë¶„ížˆ ì‹¤ì²œí•  ìˆ˜ ìžˆê¸° ë•Œë¬¸ì— ìµœëŒ€í•œ ì§€í‚¤ë„ë¡ í•©ì‹œë‹¤. '"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen('ê±´ê°•í•˜ê²Œ ì‚´ê¸° ìœ„í•œ ì„¸ ê°€ì§€ ë°©ë²•ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "1YOtvI6rpuoP",
        "outputId": "949faae7-4a6b-4074-b408-6259f696c6ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'### ì§ˆë¬¸: ìŠˆì¹´ì›”ë“œê°€ ë¬´ì—‡ì¸ê°€ìš”?\\n\\n### ë‹µë³€: ìŠˆì¹´ì›”ë“œëŠ” ìœ íŠœë¸Œ ì¸ê¸° ë°©ì†¡ì¸ìœ¼ë¡œ, ìµœì •ìƒ ê²½ì œ ìœ íŠœë²„ë¡œ í‰ê°€ë°›ê³  ìžˆìŠµë‹ˆë‹¤. ìŠˆì¹´ì™€ í•¨ê»˜í•˜ëŠ” ê²½ì œì˜ ëª¨ë“  ê²ƒì´ë¼ëŠ” íƒ€ì´í‹€ë¡œ ì§„í–‰ë˜ëŠ” ì´ ì±„ë„ì—ì„œëŠ” ì£¼ì‹, ê²½ì œì— ëŒ€í•œ ë‹¤ì–‘í•œ ì´ì•¼ê¸°ê°€ ì˜¬ë¼ì˜¤ê³  ìžˆìŠµë‹ˆë‹¤. ìŠˆì¹´ì™€ í•¨ê»˜í•˜ëŠ” ê²½ì œì˜ ëª¨ë“  ê²ƒì˜ ì±„ë„ ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\\nhttps://www.youtube.com/channel/UCZ6ny7P7lEIHN5bMljHyKQw ìŠˆì¹´ì›”ë“œwww.youtube.com ìœ íŠœë¸Œì—ì„œëŠ” ê²½ì œì— ê´€ì‹¬ì„ ê°€ì§€ì‹œëŠ” ê²ƒì´ ì¢‹ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¸ˆìœµ ê²½ì œì— ê´€ì‹¬ì´ ë§Žìœ¼ì‹¤ ê²ƒ ê°™ì€ë°, ìŠˆì¹´ê°€ ìš´ì˜í•˜ëŠ” ìµœì •ìƒ ê²½ì œ ìœ íŠœë¸Œ ìŠˆì¹´ì›”ë“œë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. íŠ¹ížˆ ìŠˆì¹´ì›”ë“œ ì˜ìƒì´ ì—…ë¡œë“œë˜ëŠ” ì›”ìš”ì¼ì€ ê¼­ ì¦ê²¨ë³´ì‹œê¸° ë°”ëžë‹ˆë‹¤. ì›”ìš”ì¼ì— ì—…ë¡œë“œë˜ëŠ” ìŠˆì¹´ì›”ë“œ ì˜ìƒì€ ìŠˆì¹´ì›”ë“œì˜ ëŒ€í‘œ ì½”ë„ˆì´ë©°, ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìžˆê¸° ë•Œë¬¸ì— ë„ì›€ì´ ë©ë‹ˆë‹¤. \\n\\nê·¸ë¦¬ê³  ìŠˆì¹´ì›”ë“œì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë“¤ë„ ì¶œì—°í•˜ê³  ìžˆìŠµë‹ˆë‹¤. \\n\\nìœ íŠœë¸Œì—ì„œ ê²€ìƒ‰ ê²°ê³¼'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen('ìŠˆì¹´ì›”ë“œê°€ ë¬´ì—‡ì¸ê°€ìš”?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "oaNYJD81sL75",
        "outputId": "bc79e6f9-6144-447c-dd3e-fead9203d64d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'### ì§ˆë¬¸: ê³µê³µì„±ì„ ì¸¡ì •í•˜ê¸°ìœ„í•œ ë°©ë²•ì€?\\n\\n### ë‹µë³€: ê³µê³µì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” ê³µê³µì„± ì²™ë„ë¥¼ êµ¬í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì¸¡ì • ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\nâ‘  ì‚¬íšŒì„±ê³¼ ê²½ì œì  íš¨ê³¼ - ê²½ì œì  ì´ìµê³¼ ì‚¬íšŒÂ·ë¬¸í™”ì  ì´ìµì„ ëª¨ë‘ ê³ ë ¤í•˜ëŠ” ì¢…í•©ì„±ê³¼ ê¸°ì¤€ì— ë”°ë¼ íŒŒì•….\\n\\nâ‘¡ ì •ì±…ì„±ê³¼ : ê³µê³µì„±ì˜ í‰ê°€ ì²™ë„ëŠ” ê³µê³µì •ì±…ì˜ íš¨ê³¼ë‚˜ ì˜í–¥ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” í˜œíƒê³¼ ê³µê³µ ì •ì±…ì— ëŒ€í•œ ê°€ì¹˜íŒë‹¨ì´ë‚˜ ì„ í˜¸ì— ì§ì ‘ì ìœ¼ë¡œ ì—°ê³„ëœ ì •ì±…ì  ìš”êµ¬ ì‚¬í•­ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤. \\n\\nâ‘¢ ì°¸ì—¬ì„±ê³¼ ê³µë™ì²´ ì˜ì‹ : ê³µê³µì„±ì˜ í‰ê°€ ì²™ë„ëŠ” ì •ì±…ê³¼ ê´€ë ¨í•œ ì‹œë¯¼ë“¤ì˜ ì§ì ‘ì°¸ì—¬ì™€ ê³µë™ì²´ ì˜ì‹, ì‹œë¯¼ë“¤ì´ ëŠë¼ëŠ” ë¬¸ì œë‚˜ ìš•êµ¬, ì‹œë¯¼ì˜ ë¬¸ì œí•´ê²° ê³¼ì •ì—ì„œì˜ ê²½í—˜ì´ë‚˜ ëŠë‚Œ ë“±ì— ëŒ€í•œ ê³µë™ì²´ ì˜ì‹ìž…ë‹ˆë‹¤. \\në”°ë¼ì„œ ê³µê³µì„±ì´ ë†’ì€ ì •ì±…ì¼ìˆ˜ë¡ ê²½ì œì„±ê³¼ ì‚¬íšŒÂ·ë¬¸í™”ì  ì¸¡ë©´, ì •ì±…ì  íš¨ê³¼ ë“± ì„¸ ê°€ì§€ ìš”ì†Œê°€ ëª¨ë‘ ë†’ì€ í‰ê°€ë¥¼ ë°›ê¸° ë•Œë¬¸ì— ê³µê³µì„±ì´ ë†’ì€ ì •ì±…ì„ ì„¸ìš°ë©´ ì •ì±…ì˜ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë˜í•œ, ê³µê³µì„±ê³¼ ê´€ë ¨í•œ ì •ì±…ì„ ë§Œë“¤ ë•ŒëŠ” ì‹œë¯¼ì˜ ì°¸ì—¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œí•˜ê³ , ì •ì±… í˜•ì„± ê³¼ì •ì—ì„œ ì‹œë¯¼ë“¤ì˜ ì˜ê²¬ì´ ì ê·¹ì ìœ¼ë¡œ ë°˜ì˜ë˜ì–´ì•¼ í•˜ëŠ” ê²ƒì´'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen('ê³µê³µì„±ì„ ì¸¡ì •í•˜ê¸°ìœ„í•œ ë°©ë²•ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOWSgQ6RuDK3",
        "outputId": "924e3e5f-ea24-4e6c-bf49-9665e5eb804c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: ì£¼ì‹ ì‹œìž¥ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•œ ë°©ë²•ì€?\n",
            "\n",
            "### ë‹µë³€: ì£¼ì‹ ì‹œìž¥ì—ì„œ ê¾¸ì¤€ížˆ ë†’ì€ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ë„ ì¢…ëª© ì„ íƒì„ ìž˜í•´ì•¼ í•©ë‹ˆë‹¤. ì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ í•˜ë½í•˜ë©´ ìˆ˜ìµë¥  ê´€ë¦¬ê°€ í•„ìš”í•˜ë©°, ì´ë¥¼ ìœ„í•´ íˆ¬ìž ìžì‚° ë°°ë¶„ ë° ê°œë³„ì¢…ëª© ì„ íƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì£¼ì‹ ì‹œìž¥ì—ì„œ ë†’ì€ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ì¢…ëª© ì„ íƒì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì—, ë³¸ì¸ì˜ íˆ¬ìž ì„±í–¥ì„ íŒŒì•…í•˜ì—¬ ì‹ ì¤‘í•œ íˆ¬ìžë¥¼ í•´ì•¼í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì£¼ì‹ íˆ¬ìžì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë¶„ì„ê³¼ ê¸°ì—… ì •ë³´, íˆ¬ìž ì „ëžµ ë“±ì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ê°œì¸ì´ ì§ì ‘ íˆ¬ìžë¥¼ í•˜ê¸° ë³´ë‹¤ëŠ” ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ì•„ íˆ¬ìží•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
            "\n",
            "ë§Œì•½, ì „ë¬¸ì ì¸ ë¶„ì„ì´ ì–´ë ¤ìš¸ ê²½ìš° ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ë©´ ê°œì¸ë„ ì‰½ê²Œ ì„±ê³µì ì¸ íˆ¬ìžë¥¼ í•  ìˆ˜ ìžˆìœ¼ë‹ˆ, ì°¸ê³ í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤. ë˜í•œ, íˆ¬ìžë¥¼ í•  ë•ŒëŠ” ì‹œìž¥ì„ ì •í™•ížˆ íŒë‹¨í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, íˆ¬ìžì˜ ìœ„í—˜ì„±ì„ ë‚®ì¶”ê¸° ìœ„í•´ì„œëŠ” ì ë¦½ì‹ íˆ¬ìž, ë¶„ì‚° íˆ¬ìž, ë¶„í•  ë§¤ìˆ˜ ê¸°ë²• ë“±ì„ í†µí•´ íˆ¬ìží•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
            "ï¿½ë”°ë¼ì„œ, ì£¼ì‹ ì‹œìž¥ì—ì„œ ë†’ì€ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´ì„œëŠ” íˆ¬ìž ì „ëžµ, ì¢…ëª© ì„ íƒ, ìœ„í—˜ ëŒ€ë¹„, ë¶„ì‚° íˆ¬ìž ë“±ì˜ ì „ëžµì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ ë°›ì•„ íˆ¬ìž\n"
          ]
        }
      ],
      "source": [
        "gen('ì£¼ì‹ ì‹œìž¥ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•œ ë°©ë²•ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPOyetf75sCv",
        "outputId": "b5e98b13-e124-494c-b254-7cdb31d56571"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: í’‹ì˜µì…˜ê³¼ ì½œì˜µì…˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ìžìžê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?\n",
            "\n",
            "### ë‹µë³€: í’‹ì´ëž€ ì£¼ì‹ì„ ì‚¬ëŠ” ê²ƒ(êµ¬ë§¤)ì„ ì˜ë¯¸í•˜ê³ , ì½œì´ëž€ ì£¼ì‹ì„ íŒŒëŠ”(íŒë§¤) ê²ƒìž…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ê¸°ì—…ì˜ ì£¼ê°€ê°€ 10ë§Œì›ì¸ë°, ê·¸ íšŒì‚¬ì—ì„œ ì‹ ê·œ ì‚¬ì—…ì„ ì§„í–‰ ì¤‘ì´ë¼, ì•žìœ¼ë¡œ ë§¤ì¶œì´ ì¦ëŒ€ë˜ì–´ ê¸°ì—… ê°€ì¹˜ê°€ ìƒìŠ¹í•  ê²ƒìœ¼ë¡œ íŒë‹¨í•´, í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ì‹ì„ ì‚¬ëŠ” ê²ƒì´ í’‹ì˜µì…˜ìž…ë‹ˆë‹¤. ì´ëŠ” ì£¼ì‹ì„ ì‚¬ëŠ” ì‹œì ê³¼ í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ê°€ê°€ í•˜ë½í•  ê²½ìš°ì—ëŠ” ê¸°ì—…ì˜ ì£¼ê°€ì™€ í•¨ê»˜ ì£¼ê°€ê°€ í•˜ë½í•˜ëŠ” ë¦¬ìŠ¤í¬ê°€ ìžˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì½œì˜µì…˜ì€ ì£¼ì‹ì„ íŒŒëŠ” ê²ƒìž…ë‹ˆë‹¤. ì´ëŠ” ì£¼ì‹ì„ íŒŒëŠ” ì‹œì ê³¼ í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ê°€ê°€ ì¦ê°€í•´ì„œ í–¥í›„ ê¸°ì—… ì „ë§ì´ ë°ì„ ê²½ìš° ì£¼ê°€ê°€ ìƒìŠ¹í•˜ë©°, í•´ë‹¹ ì£¼ì‹ì˜ í”„ë¦¬ë¯¸ì—„ì„ ë°›ì„ ìˆ˜ ìžˆëŠ” ì•ˆì •ì ì¸ íˆ¬ìžë²•ìž…ë‹ˆë‹¤. ì¼ë°˜ ê°œë¯¸ íˆ¬ìžìžë“¤ì€ ì£¼ì‹ì„ ë§¤ìˆ˜í•  ë•Œì™€ ì£¼ì‹ì„ ë§¤ë„í•  ë•Œ ë‹¤ì–‘í•œ ë¦¬ìŠ¤í¬ê°€ ì¡´ìž¬í•˜ë¯€ë¡œ, ì „ë¬¸ê°€ì˜ ì¡°ì–¸ê³¼ íˆ¬ìž ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ íˆ¬ìž ì „ëžµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤. íˆ¬ìžì— ëŒ€í•œ ìžì„¸í•œ ì‚¬í•­ì€ ì•„ëž˜ì—ì„œ ë¬´ë£Œë¡œ ì–»ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ì½œì˜µì…˜ ê±°ëž˜: ë„¤ì´ë²„ ê²€ìƒ‰ì°½ 'ì½œì˜µì…˜ ê±°ëž˜' ê²€ìƒ‰ (https://stock.naver.\n"
          ]
        }
      ],
      "source": [
        "gen('í’‹ì˜µì…˜ê³¼ ì½œì˜µì…˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ìžìžê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr9pvznHBgIZ",
        "outputId": "c46b40a4-0dff-4402-eb14-6cc8459e7e93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: í’‹ì˜µì…˜ ë§¤ë„ì™€ ì½œì˜µì…˜ ë§¤ìˆ˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ìžìžê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?\n",
            "\n",
            "### ë‹µë³€: í’‹ì˜¤ë²„(í’‹ì„ ë§¤ë„í•˜ê³  ê·¸ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ê°€ê²©ì´ ë‚®ì€ ì½œì„ ì‚¬ëŠ” ê²ƒ)ë¥¼ í•œë‹¤ê³  í•´ì„œ ë” ë†’ì€ ê°€ê²©ì˜ ì½œì„ ì‚°ë‹¤ëŠ” ì˜ë¯¸ëŠ” ì•„ë‹™ë‹ˆë‹¤. ë” ë‚®ì€ ê°€ê²©ì˜ ì½œì„ ì‚¬ëŠ” ê²ƒì´ ê¸°ë³¸ìž…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í’‹ì„ ì‚¬ëŠ” ê²ƒì€ ë§Œê¸°ê¹Œì§€ ë³´ìœ í•˜ì§€ ì•Šê³  í’‹ ë§¤ë„ í¬ì§€ì…˜ì€ íŒ” ìˆ˜ ìžˆê¸° ë•Œë¬¸ì— ë‹¨ê¸°ì ìœ¼ë¡œ ë‚®ì€ ê°€ê²©ì˜ ì½œì„ ë§¤ìˆ˜í•˜ëŠ” ê²ƒë³´ë‹¤ ë†’ì€ ê°€ê²©ì˜ ì½œì„ ë§¤ìˆ˜í•  ìˆ˜ ìžˆëŠ” ì„ íƒì˜ ë²”ìœ„ê°€ ìžˆë‹¤ê³  í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ì— ë¹„í•´ ì½œì˜¤ë²„(ì½œì„ ë§¤ë„í•˜ê³  ê·¸ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ê°€ê²©ì´ ë‚®ì€ í’‹ì„ ì‚¬ëŠ” ê²ƒ)ë¥¼ í•˜ë©´ ë§Œê¸°ê¹Œì§€ ë³´ìœ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë‹¨ê¸°ì ìœ¼ë¡œ ë†’ì€ ê°€ê²©ì˜ í’‹ì„ ì‚¬ê±°ë‚˜ ì½œì„ ì‚´ ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë§Œê¸°ê¹Œì§€ ì£¼ê°€ê°€ ìƒìŠ¹í•œë‹¤ë©´ í’‹ì˜¤ë²„ëŠ” ì½œì˜¤ë²„ì— ë¹„í•´ ë†’ì€ ìˆ˜ìµì„ ë‚¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë˜í•œ ì½œê³¼ í’‹ì˜ ê°€ê²©ì„ ë¹„êµí•´ ë³´ë©´ ë†’ì€ ê°€ê²©ì˜ ìª½ì´ ë†’ì€ ê°€ê²©ì— ë¹„í•´ ìˆ˜ìµì„±ì´ ë” ë†’ìœ¼ë¯€ë¡œ ì½œì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ì„ íƒì˜ í­ì€ ë” ë†’ì€ ê°€ê²©ì˜ ì½œì„ ì‚´ ê²ƒì¸ì§€, ì•„ë‹ˆë©´ ë” ë‚®ì€ ê°€ê²©ì˜ ì½œì„ ì‚´ ê²ƒì¸ì§€ì— ë‹¬ë ¤ ìžˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œë„ ëª…ì‹¬í•´ì•¼\n"
          ]
        }
      ],
      "source": [
        "gen('í’‹ì˜µì…˜ ë§¤ë„ì™€ ì½œì˜µì…˜ ë§¤ìˆ˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ìžìžê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYrc_CZuCrR6",
        "outputId": "8d21485b-df66-4b83-a782-9b91f5eb392c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: ë§ˆì§„ì½œì´ ë°œìƒí•˜ëŠ” ì´ìœ ê°€ ë­ì•¼? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•´ì•¼ ë§ˆì§„ì½œì„ ë§‰ì„ ìˆ˜ ìžˆì–´?\n",
            "\n",
            "### ë‹µë³€: ë§ˆì§„ì½œì´ëž€, ì„ ë¬¼/ì˜µì…˜ ë§¤ë§¤ ì‹œ ì¦ê±°ê¸ˆì´ ë¶€ì¡±í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆ ì˜ˆì¹˜ë¥¼ ìš”êµ¬ë°›ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì€ ì„ ë¬¼/ì˜µì…˜ ê±°ëž˜ ê³„ì•½ì‹œ ë¯¸ë¦¬ ì •í•´ì§„ ì•½ì •ì¦ê±°ê¸ˆì—ì„œ ì¶”ê°€ë¡œ ë¶€ë‹´í•´ì•¼ í•˜ëŠ” ì¦ê±°ê¸ˆì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í†µìƒì ìœ¼ë¡œ ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ê³„ì•½ ê¸ˆì•¡ì˜ 10%ë¥¼ ì¦ê±°ê¸ˆìœ¼ë¡œ ì˜ˆì¹˜í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” í•´ë‹¹ ê¸ˆì•¡ì˜ 3%ë¥¼ ì˜ˆì¹˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ì´ 20%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” ì´ 9%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì„ ë¬¼ ë§¤ë§¤ ì¤‘ì—ëŠ” ë§¤ì¼ ë§¤ì¼ì˜ ì‹œì„¸ì— ë”°ë¼ ì¦ê±°ê¸ˆì„ ìˆ˜ì‹œë¡œ í•„ìš”í•˜ë©´ ì¶”ê°€ë¡œ ë‚©ë¶€í•´ì•¼í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆì´ ë°œìƒí•˜ë©°, í•´ë‹¹ í•„ìš”ì•¡ ë¶€ì¡±ì‹œì—ëŠ” ë§ˆì§„ì½œì´ ë°œìƒí•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì„ ë¬¼ ê±°ëž˜ë¥¼ í•˜ê¸° ì „ì—ëŠ” ê¼­ ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì€ 'ì„ ë¬¼/ì˜µì…˜ -> ì¦ê±°ê¸ˆ -> ì¶”ê°€ì˜ˆì¹˜/ì²­ì‚°' ë°©ë²•ìœ¼ë¡œ ì¡°íšŒ ê°€ëŠ¥í•˜ë©°, ì¸í„°ë„·ì´ë‚˜ ì „í™”ë¡œë„ ì¡°íšŒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë˜, 'ì£¼ë¬¸/ì¦ê±°ê¸ˆ -> ì¶”ê°€ì¦ê±°ê¸ˆ'ì„ í†µí•´ ì¶”ê°€ì¦ê±°ê¸ˆì„ ìž…ê¸ˆí•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜\n"
          ]
        }
      ],
      "source": [
        "gen(\"ë§ˆì§„ì½œì´ ë°œìƒí•˜ëŠ” ì´ìœ ê°€ ë­ì•¼? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•´ì•¼ ë§ˆì§„ì½œì„ ë§‰ì„ ìˆ˜ ìžˆì–´?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-5etmH-fMKj"
      },
      "outputs": [],
      "source": [
        "# ### ì§ˆë¬¸: ë§ˆì§„ì½œì´ ë°œìƒí•˜ëŠ” ì´ìœ ê°€ ë­ì•¼? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•´ì•¼ ë§ˆì§„ì½œì„ ë§‰ì„ ìˆ˜ ìžˆì–´?\n",
        "\n",
        "# ### ë‹µë³€: ë§ˆì§„ì½œì´ëž€, ì„ ë¬¼/ì˜µì…˜ ë§¤ë§¤ ì‹œ ì¦ê±°ê¸ˆì´ ë¶€ì¡±í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆ ì˜ˆì¹˜ë¥¼ ìš”êµ¬ë°›ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.\n",
        "# ì¶”ê°€ì¦ê±°ê¸ˆì€ ì„ ë¬¼/ì˜µì…˜ ê±°ëž˜ ê³„ì•½ì‹œ ë¯¸ë¦¬ ì •í•´ì§„ ì•½ì •ì¦ê±°ê¸ˆì—ì„œ ì¶”ê°€ë¡œ ë¶€ë‹´í•´ì•¼ í•˜ëŠ” ì¦ê±°ê¸ˆì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "# í†µìƒì ìœ¼ë¡œ ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ê³„ì•½ ê¸ˆì•¡ì˜ 10%ë¥¼ ì¦ê±°ê¸ˆìœ¼ë¡œ ì˜ˆì¹˜í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” í•´ë‹¹ ê¸ˆì•¡ì˜ 3%ë¥¼ ì˜ˆì¹˜í•©ë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ, ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ì´ 20%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” ì´ 9%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ, ì„ ë¬¼ ë§¤ë§¤ ì¤‘ì—ëŠ” ë§¤ì¼ ë§¤ì¼ì˜ ì‹œì„¸ì— ë”°ë¼ ì¦ê±°ê¸ˆì„ ìˆ˜ì‹œë¡œ í•„ìš”í•˜ë©´ ì¶”ê°€ë¡œ ë‚©ë¶€í•´ì•¼í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆì´ ë°œìƒí•˜ë©°, í•´ë‹¹ í•„ìš”ì•¡ ë¶€ì¡±ì‹œì—ëŠ” ë§ˆì§„ì½œì´ ë°œìƒí•©ë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ, ì„ ë¬¼ ê±°ëž˜ë¥¼ í•˜ê¸° ì „ì—ëŠ” ê¼­ ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "# ì¶”ê°€ì¦ê±°ê¸ˆì€ 'ì„ ë¬¼/ì˜µì…˜ -> ì¦ê±°ê¸ˆ -> ì¶”ê°€ì˜ˆì¹˜/ì²­ì‚°' ë°©ë²•ìœ¼ë¡œ ì¡°íšŒ ê°€ëŠ¥í•˜ë©°, ì¸í„°ë„·ì´ë‚˜ ì „í™”ë¡œë„ ì¡°íšŒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
        "# ë˜, 'ì£¼ë¬¸/ì¦ê±°ê¸ˆ -> ì¶”ê°€ì¦ê±°ê¸ˆ'ì„ í†µí•´ ì¶”ê°€ì¦ê±°ê¸ˆì„ ìž…ê¸ˆí•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a91d94890634dc1bb43176347c27fec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278070bfaaac4794a649f11306ad2947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5391d6d2385549319099d3bf653c34ee",
              "IPY_MODEL_ee696f00718b43989f653dd01391a87d",
              "IPY_MODEL_e39e4f5284224bfdbde4359d8593d206"
            ],
            "layout": "IPY_MODEL_0a91d94890634dc1bb43176347c27fec"
          }
        },
        "3473e65596d5455385269f59e6159e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "388757e992344b06b2fc7e4573eba77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "416d78e5558f4019972720c8d52a410b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5391d6d2385549319099d3bf653c34ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9d84f9337948cba5562b7e995bb2ef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3473e65596d5455385269f59e6159e38",
            "value": "100%"
          }
        },
        "664f6a3720a446929197650580dd005d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e0594143bd4c2690dc3ddaa0170b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e39e4f5284224bfdbde4359d8593d206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_664f6a3720a446929197650580dd005d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_92e0594143bd4c2690dc3ddaa0170b17",
            "value": " 1/1 [00:00&lt;00:00,  9.41it/s]"
          }
        },
        "ec9d84f9337948cba5562b7e995bb2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee696f00718b43989f653dd01391a87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416d78e5558f4019972720c8d52a410b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_388757e992344b06b2fc7e4573eba77d",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
